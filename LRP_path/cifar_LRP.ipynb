{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from inverter_util import Flatten\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as Data\n",
    "from VGG_16 import VGG16\n",
    "import pickle\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "transform_train = transforms.Compose([\n",
    "#             transforms.RandomCrop(32, padding=4),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor()\n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "])\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "        root = './data/cifar-10',\n",
    "        train = True,\n",
    "        transform = transform_train,\n",
    "        download = False\n",
    ")\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "        root = './data/cifar-10',\n",
    "        train = False,\n",
    "        transform = transform_test,\n",
    "        download = False)\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def pick_neurons_layer(relev, per=0.1): \n",
    "    rel = torch.sum(relev, [2, 3])\n",
    "    print(rel)\n",
    "    _, units = torch.topk(rel, round(per*rel.size(1)), dim=1)\n",
    "    print(units.shape)\n",
    "    return units.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "tensor([ 3.5003e-03,  0.0000e+00, -9.1019e-05,  3.1745e-02,  0.0000e+00,\n",
      "         1.9547e-01,  1.0449e-01,  0.0000e+00,  0.0000e+00, -3.1143e-03,\n",
      "         0.0000e+00,  3.5061e-03,  9.3393e-04,  1.5867e-01,  3.8372e-01,\n",
      "         3.2677e-02,  2.4927e-01,  2.5888e-01,  1.4517e-03,  8.5548e-05,\n",
      "         3.4265e-04,  5.7819e-02,  1.6104e-01,  1.0073e-02,  2.0068e-01,\n",
      "         2.3496e-03,  1.3819e-02, -4.7889e-07,  1.1143e-01,  9.0164e-02,\n",
      "         0.0000e+00,  0.0000e+00, -1.3625e-03,  0.0000e+00,  0.0000e+00,\n",
      "         3.8446e-01,  8.2332e-02,  2.2673e-01,  9.9698e-01,  6.1585e-02,\n",
      "         1.8725e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.0512e-03,  3.3511e-01,  0.0000e+00,  1.8073e-01,  1.0075e-02,\n",
      "         3.4594e-01, -1.2502e-04,  1.8421e-01,  1.7827e-02,  4.4349e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.8113e-01,\n",
      "         4.6854e-04,  0.0000e+00,  9.8518e-02,  0.0000e+00])\n",
      "index: 1\n",
      "tensor([-3.0430e-03,  0.0000e+00,  1.5986e-04,  1.9509e-01,  0.0000e+00,\n",
      "         4.9941e-01,  1.6901e-01,  0.0000e+00,  0.0000e+00, -1.1431e-02,\n",
      "         0.0000e+00,  8.3546e-03,  8.8709e-02,  4.6126e-01,  5.6980e-01,\n",
      "         8.7614e-02,  7.1820e-01,  5.5526e-01,  5.4595e-03,  9.3551e-02,\n",
      "         1.3373e-03,  8.3038e-03,  3.7243e-01,  4.7354e-02,  5.7086e-01,\n",
      "         8.8574e-02,  2.9640e-02, -9.8005e-04,  2.2622e-01,  2.1045e-01,\n",
      "         0.0000e+00,  0.0000e+00, -3.9651e-04,  0.0000e+00,  0.0000e+00,\n",
      "         4.2926e-01,  1.4528e-01,  4.6304e-01,  2.0885e+00,  1.1840e-01,\n",
      "         3.7266e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -2.2761e-02,  7.5603e-01,  0.0000e+00,  3.2209e-01,  2.2777e-03,\n",
      "         9.5154e-01, -4.6346e-03,  4.8647e-01,  1.3986e-02,  8.2704e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.1535e-01,\n",
      "        -4.0539e-04,  0.0000e+00,  2.2979e-01,  0.0000e+00])\n",
      "index: 2\n",
      "tensor([ 1.8528e-05,  0.0000e+00,  3.2184e-05,  1.0820e-01, -3.8206e-07,\n",
      "         3.5079e-01,  1.2239e-01,  0.0000e+00,  0.0000e+00, -2.5633e-03,\n",
      "         0.0000e+00,  1.2133e-02,  1.2583e-02,  2.9197e-01,  4.2868e-01,\n",
      "         4.3588e-02,  3.9692e-01,  2.8070e-01,  2.5426e-03,  6.9320e-02,\n",
      "         9.6619e-04,  4.5608e-02,  2.0873e-01,  5.1946e-02,  4.0340e-01,\n",
      "         7.2586e-02,  2.3006e-02, -3.2976e-04,  1.3560e-01,  1.7978e-01,\n",
      "         0.0000e+00,  0.0000e+00, -7.7770e-05,  0.0000e+00,  0.0000e+00,\n",
      "         2.8789e-01,  1.5484e-01,  3.2480e-01,  1.3825e+00,  7.1428e-02,\n",
      "         6.4939e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.4592e-03,  4.5438e-01,  0.0000e+00,  3.4123e-01,  1.4824e-03,\n",
      "         4.3696e-01, -7.6615e-04,  3.3402e-01,  1.4804e-02,  6.4158e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4069e-01,\n",
      "        -1.8713e-03,  0.0000e+00,  1.6023e-01,  0.0000e+00])\n",
      "index: 3\n",
      "tensor([-1.4802e-03,  0.0000e+00,  2.1907e-04,  1.0075e-01,  0.0000e+00,\n",
      "         2.8119e-01,  1.1258e-01,  0.0000e+00,  0.0000e+00, -4.3250e-03,\n",
      "         0.0000e+00, -1.7706e-03,  5.2717e-02,  2.3535e-01,  3.8815e-01,\n",
      "         2.5993e-02,  3.0463e-01,  2.4931e-01,  2.0574e-03,  2.5852e-03,\n",
      "         5.6944e-04,  4.3864e-02,  2.0641e-01,  2.6543e-05,  2.6756e-01,\n",
      "         1.0176e-02,  1.7607e-02,  0.0000e+00,  7.7526e-02,  1.6542e-01,\n",
      "         0.0000e+00,  0.0000e+00, -5.7434e-04,  0.0000e+00,  0.0000e+00,\n",
      "         3.2980e-01,  7.7079e-02,  2.8332e-01,  1.5989e+00,  5.3412e-02,\n",
      "         1.7776e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -8.7751e-04,  5.1438e-01,  0.0000e+00,  1.4580e-01,  1.0587e-03,\n",
      "         4.5648e-01, -5.3618e-04,  2.3405e-01, -3.6129e-03,  4.0435e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.2884e-01,\n",
      "        -2.1163e-03,  0.0000e+00,  1.4671e-01,  0.0000e+00])\n",
      "index: 4\n",
      "tensor([-2.8869e-03,  0.0000e+00,  8.7399e-05,  1.5163e-01,  0.0000e+00,\n",
      "         4.0848e-01,  1.7840e-01,  0.0000e+00,  0.0000e+00, -8.9641e-03,\n",
      "         0.0000e+00, -4.1492e-03,  4.4149e-02,  3.9273e-01,  4.2499e-01,\n",
      "         5.7091e-02,  5.9222e-01,  5.0610e-01,  3.9269e-03,  8.8422e-02,\n",
      "         1.3826e-03,  1.2772e-02,  2.8033e-01,  4.6635e-02,  5.9703e-01,\n",
      "         1.2707e-01,  3.6085e-02, -2.1347e-03,  2.7912e-01,  2.3976e-01,\n",
      "         0.0000e+00,  0.0000e+00, -3.3644e-03,  0.0000e+00,  0.0000e+00,\n",
      "         4.9947e-01,  1.2141e-01,  5.2020e-01,  1.8472e+00,  1.3174e-01,\n",
      "         2.3187e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.6198e-02,  4.5710e-01,  0.0000e+00,  2.5626e-01, -4.0327e-03,\n",
      "         4.8731e-01, -5.0964e-03,  3.2426e-01,  2.8275e-02,  8.3623e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.4778e-01,\n",
      "        -1.7968e-04,  0.0000e+00,  1.9508e-01,  0.0000e+00])\n",
      "index: 5\n",
      "tensor([ 7.8069e-04,  0.0000e+00,  9.4343e-06,  1.6108e-02,  0.0000e+00,\n",
      "         1.4312e-01,  7.8706e-02,  0.0000e+00,  0.0000e+00, -1.5917e-03,\n",
      "         0.0000e+00,  2.9477e-04, -1.3086e-02,  1.4224e-01,  3.4113e-01,\n",
      "         4.0056e-02,  1.5192e-01,  3.3811e-01,  4.8512e-04,  3.4662e-03,\n",
      "         2.6126e-04,  1.4137e-03,  8.5146e-02,  4.8460e-04,  1.3523e-01,\n",
      "         1.0507e-02,  2.7099e-02,  0.0000e+00,  5.2110e-02,  1.8745e-01,\n",
      "         0.0000e+00,  0.0000e+00, -1.1628e-03,  0.0000e+00,  0.0000e+00,\n",
      "         6.2291e-01,  3.5687e-02,  1.4773e-01,  7.6575e-01,  4.0585e-02,\n",
      "         4.8516e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -6.5235e-04,  4.7589e-01,  0.0000e+00,  7.6324e-02,  5.0345e-02,\n",
      "         2.4825e-01,  9.2172e-03,  1.5361e-01, -1.1572e-03,  2.8169e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0259e-01,\n",
      "         1.6558e-02,  0.0000e+00,  8.9127e-02,  0.0000e+00])\n",
      "index: 6\n",
      "tensor([ 3.1120e-03,  0.0000e+00, -3.0826e-03,  1.9780e-02,  0.0000e+00,\n",
      "         2.2708e-01,  3.1105e-01,  0.0000e+00,  0.0000e+00, -4.7686e-02,\n",
      "         0.0000e+00,  1.1198e-01, -7.1378e-03,  1.6424e-01,  8.7886e-01,\n",
      "         2.8794e-02,  2.1061e-01,  1.6823e-01,  1.1655e-03,  1.2234e-01,\n",
      "         4.9253e-04,  6.7809e-01,  1.3169e-01,  1.0218e-02,  1.6905e-01,\n",
      "         2.6848e-02,  3.5471e-04, -2.9258e-03,  8.0626e-02,  3.1483e-02,\n",
      "         0.0000e+00,  0.0000e+00, -3.8489e-04,  0.0000e+00,  0.0000e+00,\n",
      "         2.7450e-01,  8.0178e-02,  2.4520e-01,  9.3243e-01,  4.7827e-02,\n",
      "        -3.2099e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -3.2861e-03,  3.9326e-01,  0.0000e+00,  1.2890e-01, -8.0216e-04,\n",
      "         8.4735e-01, -9.5437e-05,  2.3425e-01,  1.6484e-02,  4.1385e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.6545e-01,\n",
      "        -1.6589e-03,  0.0000e+00,  1.0336e-01,  0.0000e+00])\n",
      "index: 7\n",
      "tensor([-1.4749e-03,  0.0000e+00,  2.3145e-05,  6.6450e-02,  0.0000e+00,\n",
      "         3.4708e-01,  1.7258e-01,  0.0000e+00,  0.0000e+00, -6.2969e-03,\n",
      "         0.0000e+00,  2.6650e-02,  1.4512e-03,  3.6183e-01,  4.3568e-01,\n",
      "         4.9791e-02,  2.9026e-01,  3.9771e-01,  1.8393e-03,  1.6545e-02,\n",
      "         1.0621e-03,  2.8385e-01,  2.8977e-01,  2.8803e-03,  5.4245e-01,\n",
      "         4.2770e-02,  8.1779e-03, -4.0599e-04,  1.9842e-01,  2.1451e-01,\n",
      "         0.0000e+00,  0.0000e+00, -4.3994e-03,  0.0000e+00,  0.0000e+00,\n",
      "         4.2522e-01,  1.8642e-01,  4.4102e-01,  1.7308e+00,  1.1642e-01,\n",
      "        -1.3503e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.7902e-02,  4.5651e-01,  0.0000e+00,  3.4918e-01,  1.8795e-02,\n",
      "         5.1942e-01, -1.6667e-03,  3.3680e-01,  1.4845e-02,  7.4460e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.3465e-01,\n",
      "         2.3061e-03,  0.0000e+00,  1.8544e-01,  0.0000e+00])\n",
      "index: 8\n",
      "tensor([ 4.0006e-03,  0.0000e+00,  3.2463e-04,  4.9860e-02,  0.0000e+00,\n",
      "         1.6752e-01,  9.2565e-02,  0.0000e+00,  0.0000e+00, -3.9033e-03,\n",
      "         0.0000e+00,  4.7804e-02,  5.9953e-02,  1.7119e-01,  6.9792e-01,\n",
      "         2.9730e-02,  3.7787e-01,  4.5148e-01,  2.5480e-03,  3.4297e-01,\n",
      "         3.1727e-04,  6.3907e-03,  1.8905e-01,  1.1853e-01,  2.4907e-01,\n",
      "         1.0364e-01,  2.0734e-02, -3.7271e-03,  1.9969e-01,  5.7849e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         4.6486e-01,  6.3820e-02,  2.7015e-01,  7.8425e-01,  1.1387e-01,\n",
      "         7.9627e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.0824e-02,  3.9725e-01,  0.0000e+00,  1.5069e-01,  2.7984e-04,\n",
      "         4.2499e-01, -1.7580e-03,  1.7097e-01,  2.4530e-02,  5.4843e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9052e-01,\n",
      "         5.5725e-03,  0.0000e+00,  9.4771e-02,  0.0000e+00])\n",
      "index: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.5095e-04,  0.0000e+00,  1.0775e-04,  8.2093e-02,  0.0000e+00,\n",
      "         2.7202e-01,  1.0813e-01,  0.0000e+00,  0.0000e+00, -3.9389e-03,\n",
      "         0.0000e+00,  4.2196e-03,  4.9652e-02,  1.8036e-01,  3.5202e-01,\n",
      "         5.1637e-02,  4.3974e-01,  2.0347e-01,  3.0359e-03,  4.0067e-02,\n",
      "         6.4197e-04,  5.0118e-02,  1.8570e-01,  9.6376e-03,  2.3514e-01,\n",
      "         7.2905e-02,  4.3627e-03, -5.0438e-04,  7.6039e-02,  3.1971e-01,\n",
      "         0.0000e+00,  0.0000e+00, -2.1681e-04,  0.0000e+00,  0.0000e+00,\n",
      "         3.9959e-01,  1.1673e-01,  2.5678e-01,  1.2368e+00,  3.9759e-02,\n",
      "         1.3123e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -5.2654e-04,  3.7004e-01,  0.0000e+00,  2.5971e-01,  2.7883e-03,\n",
      "         3.3588e-01, -1.3915e-03,  2.8513e-01,  4.2281e-03,  5.3274e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6269e-01,\n",
      "         2.4126e-04,  0.0000e+00,  1.2195e-01,  0.0000e+00])\n",
      "index: 10\n",
      "tensor([-3.5182e-04,  0.0000e+00,  1.8281e-04,  1.5993e-02,  0.0000e+00,\n",
      "         2.4530e-01,  1.0802e-01,  0.0000e+00,  0.0000e+00,  3.8338e-04,\n",
      "         0.0000e+00,  1.5596e-02, -6.8778e-03,  1.9853e-01,  3.7052e-01,\n",
      "         1.5769e-02,  1.3870e-01,  1.9157e-01,  3.7196e-04,  2.2056e-02,\n",
      "         3.6139e-04,  1.7939e-01,  1.9011e-01,  0.0000e+00,  1.7842e-01,\n",
      "         1.0824e-02,  0.0000e+00,  0.0000e+00,  4.9945e-02,  1.0501e-01,\n",
      "         0.0000e+00,  0.0000e+00, -9.5364e-06,  0.0000e+00,  0.0000e+00,\n",
      "         3.6576e-01,  6.4103e-02,  2.7384e-01,  1.5336e+00,  3.2548e-02,\n",
      "         1.7076e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  4.8223e-01,  0.0000e+00,  1.2092e-01,  2.0034e-03,\n",
      "         3.7745e-01,  1.0262e-04,  2.6762e-01, -6.0162e-03,  3.3867e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0894e-01,\n",
      "        -1.0587e-03,  0.0000e+00,  1.2739e-01,  0.0000e+00])\n",
      "index: 11\n",
      "tensor([-7.5377e-03,  0.0000e+00,  2.1342e-04,  7.8242e-02,  0.0000e+00,\n",
      "         3.3871e-01,  1.4674e-01,  0.0000e+00,  0.0000e+00, -1.3916e-03,\n",
      "         0.0000e+00,  2.5376e-03,  1.4573e-02,  2.9443e-01,  4.6984e-01,\n",
      "         3.7715e-02,  2.4812e-01,  3.6752e-01,  2.0361e-03,  3.2820e-02,\n",
      "         1.1367e-03,  4.0189e-02,  2.0929e-01,  8.9241e-03,  5.9495e-01,\n",
      "         7.2863e-02,  2.0229e-02, -1.4281e-04,  1.4888e-01,  2.0158e-01,\n",
      "         0.0000e+00,  0.0000e+00, -5.4892e-03,  0.0000e+00,  0.0000e+00,\n",
      "         6.9518e-01,  1.7317e-01,  3.1195e-01,  1.6639e+00,  9.7872e-02,\n",
      "        -2.6983e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -8.3324e-03,  5.0166e-01,  0.0000e+00,  3.5034e-01, -2.4869e-03,\n",
      "         4.0129e-01, -1.6221e-03,  3.1309e-01, -1.0032e-02,  4.9735e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3133e-01,\n",
      "        -1.6997e-03,  0.0000e+00,  2.4922e-01,  0.0000e+00])\n",
      "index: 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a45bcac2358b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mmodel_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrue_relevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minnvestigate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_relevance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nvme/litl/CriticalPath/LRP_path/innvestigator.py\u001b[0m in \u001b[0;36minnvestigate\u001b[0;34m(self, in_tensor, rel_for_class)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                     \u001b[0mr_values_per_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_values_per_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mrelevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_propagated_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                 \u001b[0mr_values_per_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nvme/litl/CriticalPath/LRP_path/inverter_util.py\u001b[0m in \u001b[0;36mcompute_propagated_relevance\u001b[0;34m(self, layer, relevance)\u001b[0m\n\u001b[1;32m     93\u001b[0m         elif isinstance(layer,\n\u001b[1;32m     94\u001b[0m                       (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d)):\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_nd_inverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nvme/litl/CriticalPath/LRP_path/inverter_util.py\u001b[0m in \u001b[0;36mconv_nd_inverse\u001b[0;34m(self, m, relevance_in)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;31m# First and third channel repetition only contain the positive weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mout_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mout_c\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m                 \u001b[0;31m# Second and fourth channel repetition with only the negative weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seeds = [0]\n",
    "dropouts = [True]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seed in seeds:\n",
    "        for dropout in dropouts:\n",
    "            model = VGG16(num_classes=10)\n",
    "            if dropout:\n",
    "                model_path = \"../model_vgg_cifar/vgg_seed\" + str(seed) + \"_dropout_version2.pkl\"\n",
    "            else:\n",
    "                model_path = \"../model_vgg_cifar/vgg_seed\" + str(seed) + \"_nodropout_version2.pkl\"\n",
    "            checkpoint = torch.load(model_path)\n",
    "            model.load_state_dict(checkpoint)\n",
    "            model = model.cuda()\n",
    "            model.eval()\n",
    "                        # Convert to innvestigate model\n",
    "            inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                                          method=\"b-rule\",\n",
    "                                          beta=.5)\n",
    "            for i, (data, target) in enumerate(train_loader):\n",
    "\n",
    "                print(\"index:\", i)\n",
    "\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                batch_size = int(data.size()[0])\n",
    "\n",
    "                model_prediction, _ , true_relevance = inn_model.innvestigate(in_tensor=data)\n",
    "                print(true_relevance[12].sum([0,2,3]))\n",
    "                \n",
    "                tmp = torch.sum(true_relevance[0].squeeze(), [1, 2])\n",
    "\n",
    "                if i == 0:\n",
    "                    relev = true_relevance\n",
    "                else:\n",
    "                    for l in range(len(relev)):\n",
    "                        relev[l] = torch.cat((relev[l], true_relevance[l]), 0)\n",
    "            print(\"done\")\n",
    "            print(len(relev))\n",
    "            pers = [1.0]\n",
    "            for per in pers:\n",
    "                sample_neurons = {}\n",
    "                for layer in range(len(relev)):\n",
    "                    true_layer = 12-layer\n",
    "                    r = relev[true_layer]\n",
    "                    units = pick_neurons_layer(r, per)\n",
    "                    for i in range(units.shape[0]):\n",
    "                        if layer == 0:\n",
    "                            sample_neurons[i] = []\n",
    "                        sample_neurons[i].append(units[i])\n",
    "                if dropout:\n",
    "                    save_path = \"./lrp_path_\" + str(per) + \"_seed\" + str(seed) + \"_train_version2.pkl\"\n",
    "                else:\n",
    "                    save_path = \"./lrp_path_\" + str(per) + \"_seed\" + str(seed) + \"_train_nodropout_version2.pkl\"\n",
    "                output = open(save_path, 'wb')\n",
    "                pickle.dump(sample_neurons, output)  \n",
    "            print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [0]\n",
    "dropouts = [True, False]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seed in seeds:\n",
    "        for dropout in dropouts:\n",
    "            model = VGG16(num_classes=10)\n",
    "            if dropout:\n",
    "                model_path = \"../vgg_seed\" + str(seed) + \"_dropout_version2.pkl\"\n",
    "            else:\n",
    "                model_path = \"../vgg_seed\" + str(seed) + \"_nodropout_version2.pkl\"\n",
    "            checkpoint = torch.load(model_path)\n",
    "            model.load_state_dict(checkpoint)\n",
    "            model = model.cuda()\n",
    "            model.eval()\n",
    "                        # Convert to innvestigate model\n",
    "            inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                                          method=\"e-rule\",\n",
    "                                          beta=.5)\n",
    "            for i, (data, target) in enumerate(test_loader):\n",
    "\n",
    "                print(\"index:\", i)\n",
    "\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                batch_size = int(data.size()[0])\n",
    "\n",
    "                model_prediction, _ , true_relevance = inn_model.innvestigate(in_tensor=data)\n",
    "\n",
    "                if i == 0:\n",
    "                    relev = true_relevance\n",
    "                else:\n",
    "                    for l in range(len(relev)):\n",
    "                        relev[l] = torch.cat((relev[l], true_relevance[l]), 0)\n",
    "\n",
    "            print(\"done\")\n",
    "            print(len(relev))\n",
    "            pers = [0.1, 1.0]\n",
    "            for per in pers:\n",
    "                sample_neurons = {}\n",
    "                for layer in range(len(relev)):\n",
    "                    true_layer = 12-layer\n",
    "                    r = relev[true_layer]\n",
    "                    units = pick_neurons_layer(r, per)\n",
    "                    for i in range(units.shape[0]):\n",
    "                        if layer == 0:\n",
    "                            sample_neurons[i] = []\n",
    "                        sample_neurons[i].append(units[i])\n",
    "                if dropout:\n",
    "                    save_path = \"./lrp_path_\" + str(per) + \"_seed\" + str(seed) + \"_test_version2.pkl\"\n",
    "                else:\n",
    "                    save_path = \"./lrp_path_\" + str(per) + \"_seed\" + str(seed) + \"_test_nodropout_version2.pkl\"\n",
    "                output = open(save_path, 'wb')\n",
    "                pickle.dump(sample_neurons, output)  \n",
    "            print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for test_step, (val_x, val_y) in enumerate(test_loader):\n",
    "        print(\"step:\", test_step)\n",
    "        val_x = val_x.cuda()\n",
    "        val_y = val_y.cuda()\n",
    "        val_output = model(val_x)\n",
    "        _, val_pred_y = val_output.max(1)\n",
    "        if test_step == 0:\n",
    "            correct = val_pred_y.eq(val_y).sum().item()\n",
    "        else:\n",
    "            correct += val_pred_y.eq(val_y).sum().item()\n",
    "        total += val_y.size(0)\n",
    "result = float(correct) * 100.0 / float(total)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "index: 1\n",
      "index: 2\n",
      "index: 3\n",
      "done\n",
      "13\n"
     ]
    }
   ],
   "source": [
    " for i, (data, target) in enumerate(test_loader):\n",
    "    \n",
    "    print(\"index:\", i)\n",
    "\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    batch_size = int(data.size()[0])\n",
    "\n",
    "    model_prediction, _ , true_relevance = inn_model.innvestigate(in_tensor=data)\n",
    "    \n",
    "    if i == 0:\n",
    "        relev = true_relevance\n",
    "    else:\n",
    "        for l in range(len(relev)):\n",
    "            relev[l] = torch.cat((relev[l], true_relevance[l]), 0)\n",
    "\n",
    "print(\"done\")\n",
    "print(len(relev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1])\n",
      "torch.Size([10000, 1])\n",
      "torch.Size([10000, 1])\n",
      "torch.Size([10000, 1])\n",
      "torch.Size([10000, 3])\n",
      "torch.Size([10000, 3])\n",
      "torch.Size([10000, 3])\n",
      "torch.Size([10000, 5])\n",
      "torch.Size([10000, 5])\n",
      "torch.Size([10000, 5])\n",
      "torch.Size([10000, 5])\n",
      "torch.Size([10000, 5])\n",
      "torch.Size([10000, 5])\n",
      "torch.Size([10000, 2])\n",
      "torch.Size([10000, 2])\n",
      "torch.Size([10000, 4])\n",
      "torch.Size([10000, 4])\n",
      "torch.Size([10000, 8])\n",
      "torch.Size([10000, 8])\n",
      "torch.Size([10000, 8])\n",
      "torch.Size([10000, 15])\n",
      "torch.Size([10000, 15])\n",
      "torch.Size([10000, 15])\n",
      "torch.Size([10000, 15])\n",
      "torch.Size([10000, 15])\n",
      "torch.Size([10000, 15])\n",
      "torch.Size([10000, 3])\n",
      "torch.Size([10000, 3])\n",
      "torch.Size([10000, 6])\n",
      "torch.Size([10000, 6])\n",
      "torch.Size([10000, 13])\n",
      "torch.Size([10000, 13])\n",
      "torch.Size([10000, 13])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 6])\n",
      "torch.Size([10000, 6])\n",
      "torch.Size([10000, 13])\n",
      "torch.Size([10000, 13])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 51])\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "pers = [0.01, 0.03, 0.05, 0.1]\n",
    "for per in pers:\n",
    "    sample_neurons = {}\n",
    "    for layer in range(len(relev)):\n",
    "        true_layer = 12-layer\n",
    "        r = relev[true_layer]\n",
    "        units = pick_neurons_layer(r, per)\n",
    "        for i in range(units.shape[0]):\n",
    "            if layer == 0:\n",
    "                sample_neurons[i] = []\n",
    "            sample_neurons[i].append(units[i])\n",
    "\n",
    "    save_path = \"./lrp_path_\" + str(per) + \"_seed\" + str(seed) +  \"_test.pkl\"\n",
    "    output = open(save_path, 'wb')\n",
    "    pickle.dump(sample_neurons, output)  \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "index: 1\n",
      "index: 2\n",
      "index: 3\n",
      "done\n",
      "13\n",
      "torch.Size([10000, 6])\n",
      "torch.Size([10000, 6])\n",
      "torch.Size([10000, 13])\n",
      "torch.Size([10000, 13])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 26])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 51])\n",
      "torch.Size([10000, 64])\n",
      "torch.Size([10000, 64])\n",
      "torch.Size([10000, 128])\n",
      "torch.Size([10000, 128])\n",
      "torch.Size([10000, 256])\n",
      "torch.Size([10000, 256])\n",
      "torch.Size([10000, 256])\n",
      "torch.Size([10000, 512])\n",
      "torch.Size([10000, 512])\n",
      "torch.Size([10000, 512])\n",
      "torch.Size([10000, 512])\n",
      "torch.Size([10000, 512])\n",
      "torch.Size([10000, 512])\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2560\n",
    "attack = \"pgd\"\n",
    "seed = 32\n",
    "model = VGG16(num_classes=10)\n",
    "\n",
    "model_path = \"../vgg_seed\" + str(seed) + \"_dropout.pkl\"\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                                          method=\"e-rule\",\n",
    "                                          beta=.5)\n",
    "\n",
    "def read_data_label(data_path, label_path):\n",
    "    with open(data_path, 'rb') as fr:\n",
    "        test_data = pickle.load(fr)\n",
    "        size = len(test_data)\n",
    "    with open(label_path, 'rb') as fr:\n",
    "        test_label = pickle.load(fr)\n",
    "    return test_data, test_label, size\n",
    "\n",
    "def pick_neurons_layer(relev, per=0.1): \n",
    "    rel = torch.sum(relev, [2, 3])\n",
    "    _, units = torch.topk(rel, round(per*rel.size(1)), dim=1)\n",
    "    print(units.shape)\n",
    "    return units.numpy()\n",
    "\n",
    "if attack == \"pgd\":\n",
    "    test_data_path = \"../data/adversarial_samples/Vanilla/pgd/test_adv(eps_0.031).pkl\"\n",
    "    test_label_path = \"../data/adversarial_samples/Vanilla/pgd/test_label.pkl\"\n",
    "    test_data, test_label, size = read_data_label(test_data_path, test_label_path)\n",
    "    dataset = Data.TensorDataset(test_data, test_label)\n",
    "    \n",
    "data_loader = Data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i, (data, target) in enumerate(data_loader):\n",
    "    \n",
    "    print(\"index:\", i)\n",
    "\n",
    "    data, target = data.cuda(), target.cuda()\n",
    "    batch_size = int(data.size()[0])\n",
    "\n",
    "    model_prediction, _ , true_relevance = inn_model.innvestigate(in_tensor=data)\n",
    "    \n",
    "    if i == 0:\n",
    "        relev = true_relevance\n",
    "    else:\n",
    "        for l in range(len(relev)):\n",
    "            relev[l] = torch.cat((relev[l], true_relevance[l]), 0)\n",
    "\n",
    "print(\"done\")\n",
    "print(len(relev))\n",
    "\n",
    "pers = [0.1, 1.0]\n",
    "for per in pers:\n",
    "    sample_neurons = {}\n",
    "    for layer in range(len(relev)):\n",
    "        true_layer = 12-layer\n",
    "        r = relev[true_layer]\n",
    "        units = pick_neurons_layer(r, per)\n",
    "        for i in range(units.shape[0]):\n",
    "            if layer == 0:\n",
    "                sample_neurons[i] = []\n",
    "            sample_neurons[i].append(units[i])\n",
    "            \n",
    "    save_path = \"./lrp_path_\" + str(per) + \"_seed\" + str(seed) + \"_adv.pkl\"\n",
    "    \n",
    "    output = open(save_path, 'wb')\n",
    "    pickle.dump(sample_neurons, output)  \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
